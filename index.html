<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Humanoid-OmiOcc: Stereo-Based Full-View Occupancy Dataset for Embodied AI">
  <meta name="keywords" content="OmiOcc, Embodied AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Humanoid-OmiOcc: Stereo-Based Full-View Occupancy Dataset for Embodied AI</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Humanoid-OmiOcc: Stereo-Based Full-View Occupancy Dataset for Embodied AI</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Xianda Guo</a><sup>1,2,*</sup>,</span>
            <span class="author-block">
              <a href="">Bohao Zhang</a><sup>2,*</sup>,</span>
            <span class="author-block">
              <a href="">Chenwei Huang</a><sup>2,*</sup>,
            </span>
            <span class="author-block">
              <a href="">Shiyuan Chen</a><sup>3,2</sup>,
            </span>
            <span class="author-block">
              <a href="">Ruilin Wang</a><sup>4,2</sup>,
            </span>
            <span class="author-block">
              <a href="">Yiqun Duan</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="">Cong Yang</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="">Qin Zou</a><sup>1,<i class="fas fa-envelope"></i></sup>
            </span>
            <span class="author-block">
              <a href="">Wei Sui</a><sup>2,<i class="fas fa-envelope"></i></sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>School of Computer Science, Wuhan University</span>
            <span class="author-block"><sup>2</sup>D-Robotics</span>
            <span class="author-block"><sup>3</sup>Soochow University</span>
            <span class="author-block"><sup>4</sup>CASIA</span>
            <span class="author-block"><sup>5</sup>University of Technology Sydney</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (coming soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (coming soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (coming soon)</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <img src="./static/images/teaser.png"
           alt="Teaser Image"
           style="width:100%; border-radius:10px;">

      <h2 class="subtitle has-text-centered">
        Illustration of the proposed <b>Humanoid-OmniOcc</b> dataset.
        <b>Left:</b> Six representative scenes rendered in high photorealistic quality,
        covering diverse spatial layouts and material textures.
        <b>Right:</b> Visualization of one scene with four stereo RGB pairs
        (<i>Front, Rear, Left, Right</i>), their corresponding depth maps,
        and voxelized occupancy labels.
      </h2>

    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robotic perception is fundamentally based on understanding 3D geometry for safe interaction and navigation in complex environments.
            Occupancy representation, which encodes spatial occupancy states at voxel-level granularity, is a practical substrate for such decisions.
            However, existing datasets are mostly designed for autonomous driving with vehicle-centric biases (forward-facing cameras, far-field geometry, static road priors).
          </p>

          <p>
            We present <b>Humanoid-OmniOcc</b>, a panoramic stereo-based occupancy dataset tailored for humanoid perception.
            The dataset is constructed in the Isaac Sim platform with four synchronized stereo pairs mounted around a head-like rig, where it provides full-surround RGB and high-fidelity voxel ground truth via physically-based rendering and multi-view geometric verification.
          </p>

          <p>
            We further propose <b>OmiStereo</b>, a stereo-guided occupancy prediction network that leverages robust depth priors to enhance 2D-to-3D lifting and BEV reasoning. Experiments demonstrate that OmiStereo outperforms existing surround-view monocular-based methods and generalizes better to diverse environments.
          </p>

          <p>
            We hope Humanoid-OmniOcc serves as a foundation for future research in omnidirectional perception, humanoid navigation, and close-range embodied interaction.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-centered">
          <figure>
            <img src="./static/images/method.png" alt="Method Illustration" style="max-width: 100%; border-radius: 8px;">
<!--            <figcaption><b>Figure:</b> Overview of the proposed method.</figcaption>-->
          </figure>
        </div>
        <div class="content has-text-justified">
          <p>
            <b>The pipeline of our proposed OmiStereo framework.</b>
            An image encoder first extracts features from surround-view stereo images.
            These features are then processed along two decoupled pathways:
            the upper pathway lifts 2D features to 3D camera voxel features via a 2D-to-3D transformer for occupancy prediction,
            while the lower pathway estimates depth.
            Importantly, the estimated depth serves as an auxiliary input to the 2D-to-3D transformer,
            enhancing the accuracy of the view transformation.
            Finally, an occupancy head predicts the 3D occupancy grid from the camera voxel features.
          </p>
        </div>

        <div class="content has-text-centered">
          <figure>
            <img src="./static/images/figure4.png" alt="Method Illustration" style="max-width: 50%; border-radius: 8px;">
<!--            <figcaption><b>Figure:</b> Overview of the proposed method.</figcaption>-->
          </figure>
        </div>
        <div class="content has-text-justified">
          <p>
            Comparison between <b>Disparity-to-Depth Volume Model (DDVM)</b>
            and <b>Stereo Depth Network (SDN)</b> .
            The DDVM (top) first constructs a disparity cost volume and converts it into depth space
            through a channel mapping stage, while the SDN (bottom) directly builds a depth-oriented
            cost volume and performs 3D CNN with softmax regression to predict metric depth.
          </p>


      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment</h2>
        <div class="content has-text-centered">
          <figure>
            <img src="./static/images/figure5.png" alt="Method Illustration" style="max-width: 100%; border-radius: 8px;">
<!--            <figcaption><b>Figure:</b> Overview of the proposed method.</figcaption>-->
          </figure>
        </div>
        <div class="content has-text-justified">
          <p>
            <strong>Quantitative comparison on the validation set.</strong>
            IOU, Pre, and Rec denote <strong>Intersection over Union</strong>,
            <strong>Precision</strong>, and <strong>Recall</strong>, respectively.
          </p>
        </div>

        <div class="content has-text-centered">
          <figure>
            <img src="./static/images/figure6.png" alt="Method Illustration" style="max-width: 100%; border-radius: 8px;">
<!--            <figcaption><b>Figure:</b> Overview of the proposed method.</figcaption>-->
          </figure>
        </div>
        <div class="content has-text-justified">
          <p>Quantitative comparison on the <strong>test set</strong>.</p>
        </div>

      </div>
    </div>
  </div>
</section>

<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->

<!--    <div class="columns is-centered">-->

<!--      &lt;!&ndash; Visual Effects. &ndash;&gt;-->
<!--      <div class="column">-->
<!--        <div class="content">-->
<!--          <h2 class="title is-3">Visual Effects</h2>-->
<!--          <p>-->
<!--            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect-->
<!--            would be impossible without nerfies since it would require going through a wall.-->
<!--          </p>-->
<!--          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/dollyzoom-stacked.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--      &lt;!&ndash;/ Visual Effects. &ndash;&gt;-->

<!--      &lt;!&ndash; Matting. &ndash;&gt;-->
<!--      <div class="column">-->
<!--        <h2 class="title is-3">Matting</h2>-->
<!--        <div class="columns is-centered">-->
<!--          <div class="column content">-->
<!--            <p>-->
<!--              As a byproduct of our method, we can also solve the matting problem by ignoring-->
<!--              samples that fall outside of a bounding box during rendering.-->
<!--            </p>-->
<!--            <video id="matting-video" controls playsinline height="100%">-->
<!--              <source src="./static/videos/matting.mp4"-->
<!--                      type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->

<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--    &lt;!&ndash;/ Matting. &ndash;&gt;-->

<!--    &lt;!&ndash; Animation. &ndash;&gt;-->
<!--    <div class="columns is-centered">-->
<!--      <div class="column is-full-width">-->
<!--        <h2 class="title is-3">Animation</h2>-->

<!--        &lt;!&ndash; Interpolating. &ndash;&gt;-->
<!--        <h3 class="title is-4">Interpolating states</h3>-->
<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            We can also animate the scene by interpolating the deformation latent codes of two input-->
<!--            frames. Use the slider here to linearly interpolate between the left frame and the right-->
<!--            frame.-->
<!--          </p>-->
<!--        </div>-->
<!--        <div class="columns is-vcentered interpolation-panel">-->
<!--          <div class="column is-3 has-text-centered">-->
<!--            <img src="./static/images/interpolate_start.jpg"-->
<!--                 class="interpolation-image"-->
<!--                 alt="Interpolate start reference image."/>-->
<!--            <p>Start Frame</p>-->
<!--          </div>-->
<!--          <div class="column interpolation-video-column">-->
<!--            <div id="interpolation-image-wrapper">-->
<!--              Loading...-->
<!--            </div>-->
<!--            <input class="slider is-fullwidth is-large is-info"-->
<!--                   id="interpolation-slider"-->
<!--                   step="1" min="0" max="100" value="0" type="range">-->
<!--          </div>-->
<!--          <div class="column is-3 has-text-centered">-->
<!--            <img src="./static/images/interpolate_end.jpg"-->
<!--                 class="interpolation-image"-->
<!--                 alt="Interpolation end reference image."/>-->
<!--            <p class="is-bold">End Frame</p>-->
<!--          </div>-->
<!--        </div>-->
<!--        <br/>-->
<!--        &lt;!&ndash;/ Interpolating. &ndash;&gt;-->

<!--        &lt;!&ndash; Re-rendering. &ndash;&gt;-->
<!--        <h3 class="title is-4">Re-rendering the input video</h3>-->
<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel-->
<!--            viewpoint such as a stabilized camera by playing back the training deformations.-->
<!--          </p>-->
<!--        </div>-->
<!--        <div class="content has-text-centered">-->
<!--          <video id="replay-video"-->
<!--                 controls-->
<!--                 muted-->
<!--                 preload-->
<!--                 playsinline-->
<!--                 width="75%">-->
<!--            <source src="./static/videos/replay.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        &lt;!&ndash;/ Re-rendering. &ndash;&gt;-->

<!--      </div>-->
<!--    </div>-->
<!--    &lt;!&ndash;/ Animation. &ndash;&gt;-->


<!--    &lt;!&ndash; Concurrent Work. &ndash;&gt;-->
<!--    <div class="columns is-centered">-->
<!--      <div class="column is-full-width">-->
<!--        <h2 class="title is-3">Related Links</h2>-->

<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            There's a lot of excellent work that was introduced around the same time as ours.-->
<!--          </p>-->
<!--          <p>-->
<!--            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.-->
<!--          </p>-->
<!--          <p>-->
<!--            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>-->
<!--            both use deformation fields to model non-rigid scenes.-->
<!--          </p>-->
<!--          <p>-->
<!--            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>-->
<!--          </p>-->
<!--          <p>-->
<!--            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--    &lt;!&ndash;/ Concurrent Work. &ndash;&gt;-->

<!--  </div>-->
<!--</section>-->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{humannoid-omniocc,
  author    = {Xianda Guo, Bohao Zhang, Chenwei Huang, Shiyuan Chen, Ruilin Wang, Yiqun Duan, Cong Yang, Qin Zou, Wei Sui},
  title     = {Humanoid-OmiOcc: Stereo-Based Full-View Occupancy Dataset for Embodied AI},
  journal   = {XXX},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
